The files present in this folder illustrate how to post train quantize the models obtained from Knowledge Distillation and Pruning or after training with FP32 precision. The models saved after pruning and student model with distilled knowledge can be quantized to obtain further compressed model with better speedup and almost similar accuracy. 

For using this code, make appropriate changes. Refer the commented examples for understanding.
